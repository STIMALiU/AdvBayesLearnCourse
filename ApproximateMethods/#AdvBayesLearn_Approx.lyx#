#LyX 2.1 created this file. For more info see http://www.lyx.org/
\lyxformat 474
\begin_document
\begin_header
\textclass beamer
\begin_preamble
\setcounter{MaxMatrixCols}{10}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{mathpazo}
\usepackage{hyperref}
\usepackage{multimedia}
\usepackage{xcolor}
\usepackage{colortbl}
\definecolor{RawSienna}{cmyk}{0,0.87,0.82,0.31}
\definecolor{gray97}{cmyk}{0,0,0,0.03}
\definecolor{robinsegg}{cmyk}{0.18,0.04,0,0.07}
\definecolor{cola}{cmyk}{0,0.315,0.35,0.155}

\newenvironment{stepenumerate}{\begin{enumerate}[<+->]}{\end{enumerate}}
\newenvironment{stepitemize}{\begin{itemize}[<+->]}{\end{itemize} }
\newenvironment{stepenumeratewithalert}{\begin{enumerate}[<+-| alert@+>]}{\end{enumerate}}
\newenvironment{stepitemizewithalert}{\begin{itemize}[<+-| alert@+>]}{\end{itemize} }
\usecolortheme[named=RawSienna]{structure}

\usepackage{algorithm2e}

%\usecolortheme[RGB={205,0,0}]{structure}
\setbeamertemplate{navigation symbols}{}
\useoutertheme{infolines}
\usetheme{default}
\setbeamertemplate{blocks}[shadow=true]
%\setbeamerfont{structure}{shape=\itshape}
\usefonttheme{structuresmallcapsserif}
\setbeamertemplate{background canvas}{
 % \ifnum \thepage>0 \relax % we are on the first page
%\includegraphics[width=\paperwidth,height=\paperheight]{/home/mv/Dropbox/Foton/IconsWallpaper/greyribbonLighter.jpg}
 % \else
 	% No background for page 2 and onwards
 % \fi
}
\end_preamble
\options xcolor=svgnames
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman palatino
\font_sans default
\font_typewriter default
\font_math auto
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 0
\use_package esint 0
\use_package mathdots 1
\use_package mathtools 0
\use_package mhchem 1
\use_package stackrel 0
\use_package stmaryrd 0
\use_package undertilde 0
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Adv Bayesian Learning
\end_layout

\end_inset

Advanced Bayesian Learning
\begin_inset Newline newline
\end_inset

Approximate Methods
\begin_inset Newline newline
\end_inset

Spring 2014
\end_layout

\begin_layout Author
\begin_inset Argument 1
status open

\begin_layout Plain Layout
Mattias Villani
\end_layout

\end_inset

Mattias Villani
\end_layout

\begin_layout Institute

\series bold
\begin_inset Argument 1
status open

\begin_layout Plain Layout

\series bold
Statistics, LiU
\end_layout

\end_inset

Division of Statistics
\begin_inset Newline newline
\end_inset

Department of Computer and Information Science
\begin_inset Newline newline
\end_inset

Link√∂ping University 
\end_layout

\begin_layout Date
\begin_inset space \thinspace{}
\end_inset


\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Topic overview
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Variational Bayes (VB)
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
Approximate Bayesian Computations (ABC)
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
Integrated Nested Laplace Approximation (INLA)
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Variational Bayes
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Let 
\begin_inset Formula $\theta=(\theta_{1},...,\theta_{M})$
\end_inset

.
 Approximate the posterior 
\begin_inset Formula $p(\theta|y)$
\end_inset

 with a (simpler) distribution 
\begin_inset Formula $q(\theta)$
\end_inset

.
 
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Nonparametric/Factorization/Mean field approximation
\series default

\begin_inset Formula 
\[
q(\theta)=\prod_{i=1}^{M}q_{i}(\theta_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Parametric
\series default
, where 
\begin_inset Formula $q_{\lambda}(\theta)$
\end_inset

 is a parametric family with parameters 
\begin_inset Formula $\lambda$
\end_inset

.
 
\end_layout

\end_deeper
\begin_layout Itemize
Find the 
\begin_inset Formula $q(\theta)$
\end_inset

 that 
\series bold
minimizes the Kullback-Leibler distance
\series default
:
\begin_inset Formula 
\[
KL(p,q)=\int p(\theta|y)\ln\frac{p(\theta|y)}{q(\theta)}d\theta=E_{p}\left[\ln\frac{p(\theta|y)}{q(\theta)}\right].
\]

\end_inset


\end_layout

\begin_layout Itemize
Computing the expectation wrt 
\begin_inset Formula $p(\theta|y)$
\end_inset

 is often hard.
 
\end_layout

\begin_layout Itemize

\series bold
Reverse KL 
\series default
problem is often simpler (but somewhat unnatural):
\begin_inset Formula 
\[
KL(q,p)=\int q(\theta)\ln\frac{q(\theta)}{p(\theta|y)}d\theta=E_{q}\left[\ln\frac{q(\theta)}{p(\theta|y)}\right].
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
VB gives a lower bound on 
\begin_inset Formula $p(y)$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Using that 
\begin_inset Formula $p(\theta|\mathbf{y})=p(\mathbf{y},\theta)/p(\mathbf{y})$
\end_inset

 we have
\begin_inset Formula 
\begin{align*}
KL(q,p) & =\int q(\theta)\ln\frac{q(\theta)}{p(\theta|\mathbf{y})}d\theta\\
 & =\int q(\theta)\ln\left(\frac{q(\theta)}{p(\mathbf{y},\theta)}\right)d\theta+\int q(\theta)\ln p(\mathbf{y})d\theta\\
 & =\int q(\theta)\ln\frac{q(\theta)}{p(\mathbf{y},\theta)}d\theta+\ln p(\mathbf{y})
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Since 
\begin_inset Formula $KL(q,p)\geq0$
\end_inset

, we have the following 
\series bold
lower bound
\series default
 for 
\begin_inset Formula $\ln p(\mathbf{y})$
\end_inset

 
\begin_inset Formula 
\[
\ln p(\mathbf{y})\geq-\int q(\theta)\ln\frac{q(\theta)}{p(\mathbf{y},\theta)}d\theta=\int q(\theta)\ln\frac{p(\mathbf{y},\theta)}{q(\theta)}d\theta\overset{def}{=}\ln\underline{p}(\mathbf{y};q),
\]

\end_inset

where 
\begin_inset Formula $p(\mathbf{y},\theta)=p(\mathbf{y}|\theta)p(\theta)$
\end_inset

 is the unnormalized posterior.
\end_layout

\begin_layout Itemize
Minimizing 
\begin_inset Formula $KL(q,p)$
\end_inset

 is the same as maximizing 
\begin_inset Formula $\ln\underline{p}(\mathbf{y};q)$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mean field approximation
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Factorization
\begin_inset Formula 
\[
q(\theta)=\prod_{i=1}^{M}q_{i}(\theta_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
No functional forms are assumed
\series default
 for the 
\begin_inset Formula $q_{i}(\theta)$
\end_inset

.
 Nonparametric.
\end_layout

\begin_layout Itemize

\series bold
Optimal densities
\series default
 can be shown to satisfy:
\begin_inset Formula 
\[
q_{i}(\theta)\propto\exp\left(E_{-\theta_{i}}\ln p(\mathbf{y},\theta)\right)
\]

\end_inset

where 
\begin_inset Formula $E_{-\theta_{i}}(\cdot)$
\end_inset

 is the expectation with respect to 
\begin_inset Formula $\prod_{i\neq j}q_{j}(\theta_{j})$
\end_inset

.
\end_layout

\begin_layout Itemize
Alternative formulation that 
\series bold
connects to Gibbs sampling
\series default

\begin_inset Formula 
\[
q_{i}(\theta)\propto\exp\left(E_{-\theta_{i}}\ln p(\theta_{i}|\mathrm{rest})\right)
\]

\end_inset

where 
\begin_inset Formula $p(\theta_{i}|\mathrm{rest})$
\end_inset

 is the full conditional posterior of 
\begin_inset Formula $\theta_{i}$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Structured mean field approximation
\series default
.
 Group subset of parameters in tractable blocks.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mean field approximation - algorithm
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Initialize: 
\begin_inset Formula $q_{2}^{*}(\theta_{2}),...,q_{M}^{*}(\theta_{M})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Repeat until increase in 
\begin_inset Formula $\ln\underline{p}(\mathbf{y};q)$
\end_inset

 is negligible:
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $q_{1}^{*}(\theta_{1})\leftarrow\frac{\exp\left[E_{-\theta_{1}}\ln p(\mathbf{y},\theta)\right]}{\int\exp\left[E_{-\theta_{1}}\ln p(\mathbf{y},\theta)\right]d\theta_{1}}$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\vdots$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $q_{M}^{*}(\theta_{M})\leftarrow\frac{\exp\left[E_{-\theta_{M}}\ln p(\mathbf{y},\theta)\right]}{\int\exp\left[E_{-\theta_{M}}\ln p(\mathbf{y},\theta)\right]d\theta_{M}}$
\end_inset


\begin_inset VSpace bigskip
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Note: we make no assumptions about parametric form of the 
\begin_inset Formula $q_{i}(\theta)$
\end_inset

, but the optimal 
\begin_inset Formula $q_{i}(\theta)$
\end_inset

 often turn out to be parametric (normal, gamma etc).
 
\end_layout

\begin_layout Itemize
The updates above then boil down to just updating of hyperparameters in
 the optimal densities.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Mean field approximation - Normal model
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Model
\series default
: 
\begin_inset Formula $X_{i}|\mu,\sigma^{2}\overset{iid}{\sim}N(\mu,\sigma^{2})$
\end_inset

.
\end_layout

\begin_layout Itemize

\series bold
Prior
\series default
: 
\begin_inset Formula $\mu\sim N(\mu_{\mu},\sigma_{\mu}^{2})$
\end_inset

 
\series bold
independent
\series default
 of 
\begin_inset Formula $\sigma^{2}\sim IG(A,B)$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Note: this is NOT the conjugate prior.
\end_layout

\begin_layout Itemize

\series bold
Variational approximation
\series default
: 
\begin_inset Formula $q(\mu,\sigma^{2})=q_{\mu}(\mu)\cdot q_{\sigma^{2}}(\sigma^{2})$
\end_inset

.
\end_layout

\begin_layout Itemize
Optimal densities
\begin_inset Formula 
\begin{align*}
q_{\mu}^{*}(\mu) & \propto\exp\left[E_{q(\sigma^{2})}\ln p(\mu|\sigma^{2},\mathbf{x})\right]\\
q_{\sigma^{2}}^{*}(\sigma^{2}) & \propto\exp\left[E_{q(\mu)}\ln p(\sigma^{2}|\mu,\mathbf{x})\right]
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Full conditional posteriors
\begin_inset Formula 
\begin{align*}
\mu|\sigma^{2},\mathbf{x} & \sim N\left(\frac{n\bar{x}/\sigma^{2}+\mu_{\mu}/\sigma_{\mu}^{2}}{n/\sigma^{2}+1/\sigma_{\mu}^{2}},\frac{1}{n/\sigma^{2}+1/\sigma_{\mu}^{2}}\right)\\
\sigma^{2}|\mu,\mathbf{x} & \sim IG\left(A+\frac{n}{2},B+\frac{1}{2}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)\right)
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal model example - Updating 
\begin_inset Formula $q_{\sigma^{2}}^{*}(\sigma^{2})$
\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Full conditional posterior of 
\begin_inset Formula $\sigma^{2}$
\end_inset


\begin_inset Formula 
\[
\sigma^{2}|\mu,\mathbf{x}\sim IG\left(A+\frac{n}{2},B+\frac{1}{2}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
So, 
\begin_inset Formula $E_{q(\mu)}\ln p(\sigma^{2}|\mu,\mathbf{x})$
\end_inset

 is proportional to
\begin_inset Formula 
\begin{align*}
E_{q(\mu)}\left[-\left(A+\frac{n}{2}+1\right)\ln\sigma^{2}-\left(B+\frac{1}{2}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)\right)/\sigma^{2}\right]
\end{align*}

\end_inset

and therefore 
\begin_inset Formula 
\[
q_{\sigma^{2}}^{*}(\sigma^{2})\propto\left(\sigma^{2}\right)^{-(A+n/2+1)}\exp\left(-\left(B+\frac{1}{2}E_{q(\mu)}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)/\sigma^{2}\right)\right)
\]

\end_inset

which shows that 
\begin_inset Formula $q_{\sigma^{2}}^{*}(\sigma^{2})$
\end_inset

 is 
\begin_inset Formula 
\[
IG\left(A+\frac{n}{2},B+\frac{1}{2}E_{q(\mu)}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)\right)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal model example - Updating 
\begin_inset Formula $q_{\sigma^{2}}^{*}(\sigma^{2})$
\end_inset


\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So 
\begin_inset Formula $q_{\sigma^{2}}^{*}(\sigma^{2})$
\end_inset

 is 
\begin_inset Formula 
\[
IG\left(A+\frac{n}{2},B+\frac{1}{2}E_{q(\mu)}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)\right)
\]

\end_inset

but what is 
\begin_inset Formula $E_{q(\mu)}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)$
\end_inset

?
\begin_inset Formula 
\begin{align*}
 & E_{q(\mu)}(\mathbf{x}-\mu)'(\mathbf{x}-\mu)=\\
 & E_{q(\mu)}\left[\left(\mathbf{x}-E_{q(\mu)}\mu\right)+\left(E_{q(\mu)}\mu-\mu\right)\mathbf{1}_{n}\right]'\left[\left(\mathbf{x}-E_{q(\mu)}(\mu)\right)+\left(E_{q(\mu)}(\mu)-\mu\right)\mathbf{1}_{n}\right]\\
 & =E_{q(\mu)}\left[\left(\mathbf{x}-E_{q(\mu)}(\mu)\right)'\left(\mathbf{x}-E_{q(\mu)}(\mu)\right)+\left(E_{q(\mu)}(\mu)-\mu\right)^{2}n\right]\\
 & =\left(\mathbf{x}-E_{q(\mu)}(\mu)\right)'\left(\mathbf{x}-E_{q(\mu)}(\mu)\right)+n\cdot Var_{q(\mu)}(\mu)
\end{align*}

\end_inset

because 
\begin_inset Formula $E_{q(\mu)}\left(E_{q(\mu)}(\mu)-\mu\right)=0$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Important: 
\begin_inset Formula $E_{q(\mu)}(\mu)$
\end_inset

 and 
\begin_inset Formula $Var_{\mu}(\mu)$
\end_inset

 is the mean and variance of the 
\begin_inset Formula $q_{\mu}$
\end_inset

 distribution.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal model example - Updating 
\begin_inset Formula $q_{\mu}^{*}(\mu)$
\end_inset

 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Easier to go back to the form 
\begin_inset Formula $q_{\mu}(\mu)\propto\exp\left(E_{\sigma^{2}}\ln p(\mathbf{y},\mu,\sigma^{2})\right)$
\end_inset

, where 
\begin_inset Formula 
\begin{align*}
\ln p(\mathbf{y},\mu,\sigma^{2}) & =\ln p(\mathbf{y}|\mu,\sigma^{2})+\ln p(\mu)+\ln p(\sigma^{2})\\
 & =-\frac{1}{2\sigma^{2}}\sum_{i=1}^{n}(x_{i}-\mu)^{2}-\frac{1}{2\sigma_{\mu}^{2}}(\mu-\mu_{\mu})^{2}+const
\end{align*}

\end_inset


\begin_inset Formula 
\[
q_{\mu}(\mu)\propto\exp\left(-E_{\sigma^{2}}\left(\frac{1}{\sigma^{2}}\right)\frac{1}{2}\sum_{i=1}^{n}(x_{i}-\mu)^{2}-\frac{1}{2\sigma_{\mu}^{2}}(\mu-\mu_{\mu})^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
Completing the square shows
\begin_inset Formula 
\[
q_{\mu}(\mu)=N\left(E_{\mu}(\mu),Var_{\mu}(\mu)\right)
\]

\end_inset

where 
\begin_inset Formula 
\[
E_{\mu}(\mu)=\frac{n\bar{x}E_{\sigma^{2}}\left(\frac{1}{\sigma^{2}}\right)+\mu_{\mu}/\sigma_{\mu}^{2}}{nE_{\sigma^{2}}\left(\frac{1}{\sigma^{2}}\right)+1/\sigma_{\mu}^{2}}
\]

\end_inset

and 
\begin_inset Formula 
\[
Var_{\mu}(\mu)=\frac{1}{nE_{\sigma^{2}}\left(\frac{1}{\sigma^{2}}\right)+1/\sigma_{\mu}^{2}}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal model example - summary 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Variational density for 
\begin_inset Formula $\sigma^{2}$
\end_inset


\begin_inset Formula 
\[
q_{\sigma^{2}}(\sigma^{2})=IG\left(A_{q},B_{q}\right)
\]

\end_inset

where 
\begin_inset Formula $A_{q}=A+n/2$
\end_inset

 and 
\begin_inset Formula $B_{q}=B+\frac{1}{2}\left(\left\Vert \mathbf{x}-\mu_{q}\cdot\mathbf{1}_{n}\right\Vert ^{2}+n\cdot\sigma_{q}^{2}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Variational density for 
\begin_inset Formula $\mu$
\end_inset


\begin_inset Formula 
\[
q_{\mu}(\mu)=N\left(\mu_{q},\sigma_{q}^{2}\right)
\]

\end_inset

where
\begin_inset Formula 
\[
\sigma_{q}^{2}=\frac{1}{n\frac{A_{q}}{B_{q}}+1/\sigma_{\mu}^{2}}
\]

\end_inset


\begin_inset Formula 
\[
\mu_{q}=\left(n\bar{x}\frac{A_{q}}{B_{q}}+\mu_{\mu}/\sigma_{\mu}^{2}\right)\sigma_{q}^{2}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Frame

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Normal model example - algorithm 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Set 
\begin_inset Formula $A_{q}=A+n/2$
\end_inset

.
 
\end_layout

\begin_layout Itemize
Initialize 
\begin_inset Formula $E_{\mu}(\mu)=\bar{x}$
\end_inset

 and 
\begin_inset Formula $Var_{\mu}(\mu)=s^{2}/n$
\end_inset

.
\end_layout

\begin_layout Itemize
Repeat
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula 
\[
B_{q}\leftarrow B+\frac{1}{2}\left(\left\Vert \mathbf{x}-\mu_{q}\cdot\mathbf{1}_{n}\right\Vert ^{2}+n\cdot\sigma_{q}^{2}\right)
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
\sigma_{q}^{2}\leftarrow\frac{1}{n\frac{A_{q}}{B_{q}}+1/\sigma_{\mu}^{2}}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula 
\[
\mu_{q}\leftarrow\left(n\bar{x}\frac{A_{q}}{B_{q}}+\mu_{\mu}/\sigma_{\mu}^{2}\right)\sigma_{q}^{2}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Until the change in 
\begin_inset Formula 
\[
\ln\underline{p}(\mathbf{x};q)=\frac{1}{2}-\frac{n}{2}\log(2\pi)+\frac{1}{2}\ln\left(\sigma_{q}^{2}/\sigma_{\mu}^{2}\right)-\frac{(\mu_{q}-\mu_{\mu})^{2}+\sigma_{q}^{2}}{2\sigma_{\mu}^{2}}
\]

\end_inset

is negligible.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probit regression 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Model
\series default
: 
\begin_inset Formula 
\[
Y_{i}|x_{i}\overset{ind.}{\sim}Bern\left[\Phi(x_{i}'\beta)\right]
\]

\end_inset


\end_layout

\begin_layout Itemize

\series bold
Prior
\series default
: 
\begin_inset Formula $\beta\sim N(\mu_{\beta},\Sigma_{\beta})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Latent variable formulation
\series default
 with 
\begin_inset Formula $\mathbf{a}=(a_{1},...,a_{n})'$
\end_inset


\begin_inset Formula 
\[
\mathbf{a}|\beta\sim N(X\beta,1)
\]

\end_inset

and
\begin_inset Formula 
\[
p(y_{i}|a_{i})=I(a_{i}\geq0)^{y_{i}}I(a_{i}<0)^{1-y_{i}}
\]

\end_inset


\end_layout

\begin_layout Itemize
Factorized 
\series bold
variational approximation
\series default

\begin_inset Formula 
\[
q(\mathbf{a},\beta)=q_{\mathbf{a}}(\mathbf{a})q_{\beta}(\beta)
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probit regression - updating 
\series bold
\size largest
\color blue
a
\series default
\size default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Log joint distribution
\begin_inset Formula 
\begin{align*}
\ln p(y,\beta,\mathbf{a}) & =\ln p(y|\beta,\mathbf{a})+\ln p(\mathbf{a}|\beta)+\ln p(\beta)\\
 & \propto\sum_{i=1}^{n}\left(y_{i}\ln I(a_{i}\geq0)+(1-y_{i})\ln I(a_{i}<0)\right)\\
 & -\frac{1}{2}(\mathbf{a}-\mathbf{X}\beta)'(\mathbf{a}-\mathbf{X}\beta)-\frac{1}{2}(\beta-\mu_{\beta})'\Sigma_{\beta}(\beta-\mu_{\beta})
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Updating 
\begin_inset Formula $\mathbf{a}$
\end_inset


\begin_inset Formula 
\begin{align*}
\ln q_{\mathbf{a}}(\mathbf{a}) & \propto E_{q(\beta)}\ln p(y,\beta,\mathbf{a})\propto\sum_{i=1}^{n}\left(y_{i}\ln I(a_{i}\geq0)+(1-y_{i})\ln I(a_{i}<0)\right)\\
 & -\frac{1}{2}E_{q(\beta)}(\mathbf{a}-\mathbf{X}\beta)'(\mathbf{a}-\mathbf{X}\beta)
\end{align*}

\end_inset

Note that 
\begin_inset Formula 
\begin{align*}
E_{q(\beta)}(\mathbf{a}-\mathbf{X}\beta)'(\mathbf{a}-\mathbf{X}\beta) & =\mathbf{a}'\mathbf{a}-2\mathbf{a}'\mathbf{X}E_{q(\beta)}(\beta)+const\\
 & =(\mathbf{a}-\mathbf{X}E_{q(\beta)}(\beta))'(\mathbf{a}-\mathbf{X}E_{q(\beta)}(\beta))+const
\end{align*}

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probit regression - updating 
\series bold
\size largest
\color blue
a
\series default
\size default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
So
\begin_inset Formula 
\begin{align*}
q_{\mathbf{a}}(\mathbf{a}) & \propto\prod_{i=1}^{n}I(a_{i}\geq0)^{y_{i}}I(a_{i}<0)^{1-y_{i}}\\
 & \times\exp\left(-\frac{1}{2}(\mathbf{a}-\mathbf{X}\mu_{q(\beta)})'(\mathbf{a}-\mathbf{X}\mu_{q(\beta)})\right)
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
Normalizing gives
\begin_inset Formula 
\begin{align*}
q_{\mathbf{a}}(\mathbf{a}) & =\prod_{i=1}^{n}\left[\frac{I(a_{i}\geq0)}{\Phi\left((\mathbf{X}\mu_{q(\beta)})_{i}\right)}\right]^{y_{i}}\left[\frac{I(a_{i}<0)}{1-\Phi\left((\mathbf{X}\mu_{q(\beta)})_{i}\right)}\right]^{1-y_{i}}\\
 & \times(2\pi)^{-n/2}\exp\left(-\frac{1}{2}\left\Vert \mathbf{a}-\mathbf{X}\mu_{q(\beta)}\right\Vert ^{2}\right)
\end{align*}

\end_inset

which is a product of 
\begin_inset Formula $n$
\end_inset

 truncated normals.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probit regression - updating
\series bold
\size largest
\color blue
 
\begin_inset Formula $\beta$
\end_inset


\series default
\size default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Updating 
\begin_inset Formula $\beta$
\end_inset

 
\begin_inset Formula 
\begin{align*}
\ln q_{\beta}(\beta) & \propto E_{q(\mathbf{a})}\ln p(y,\beta,\mathbf{a})\\
 & \propto-\frac{1}{2}E_{q(\mathbf{a})}(\mathbf{a}-\mathbf{X}\beta)'(\mathbf{a}-\mathbf{X}\beta)-\frac{1}{2}(\beta-\mu_{\beta})'\Sigma_{\beta}(\beta-\mu_{\beta})
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
For any random vector 
\begin_inset Formula $\mathbf{y}$
\end_inset

 with mean 
\begin_inset Formula $\mu$
\end_inset

 and covariance matrix 
\begin_inset Formula $\Omega$
\end_inset


\begin_inset Formula 
\[
E(\mathbf{y}-\mathbf{m})'(\mathbf{y}-\mathbf{m})=trace(\Omega)+(\mu-\mathbf{m})'(\mu-\mathbf{m})
\]

\end_inset


\end_layout

\begin_layout Itemize
So
\begin_inset Formula 
\begin{align*}
\ln q_{\beta}(\beta) & \propto-\frac{1}{2}trace(\Sigma_{\mathbf{a}})+(\mu_{\mathbf{a}}-\mathbf{X}\beta)'(\mu_{\mathbf{a}}-\mathbf{X}\beta)\\
 & -\frac{1}{2}(\beta-\mu_{\beta})'\Sigma_{\beta}(\beta-\mu_{\beta})
\end{align*}

\end_inset


\end_layout

\begin_layout Itemize
The variational approximation of 
\begin_inset Formula $\beta$
\end_inset

 is like the posterior from regressing 
\begin_inset Formula $\mu_{\mathbf{a}}$
\end_inset

 on 
\begin_inset Formula $\mathbf{X}$
\end_inset

 with prior 
\begin_inset Formula $\beta\sim N(\mu_{\beta},\Sigma_{\beta})$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Probit regression - updating
\series bold
\size largest
\color blue
 
\begin_inset Formula $\beta$
\end_inset


\series default
\size default
\color inherit
 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
We therefore have
\begin_inset Formula 
\[
q_{\beta}(\beta)=N\left(\mu_{q(\beta)},\left(\mathbf{X}'\mathbf{X}+\Sigma_{\beta}^{-1}\right)^{-1}\right)
\]

\end_inset

and 
\begin_inset Formula 
\[
\mu_{q(\beta)}=\left(\mathbf{X}'\mathbf{X}+\Sigma_{\beta}^{-1}\right)^{-1}\left(\mathbf{X}'\mu_{\mathbf{a}}+\Sigma_{\beta}^{-1}\mu_{\beta}\right)
\]

\end_inset

where 
\begin_inset Formula 
\[
\mu_{\mathbf{a}}=X\mu_{q(\beta)}+\frac{\phi\left(X\mu_{q(\beta)}\right)}{\Phi\left(X\mu_{q(\beta)}\right)^{\mathbf{y}}\left[\Phi\left(X\mu_{q(\beta)}\right)-\mathbf{1}_{n}\right]^{\mathbf{1_{n}-y}}}
\]

\end_inset

which follows from the expected value formula for a truncated distribution.
\end_layout

\begin_layout Itemize
The lower bound 
\begin_inset Formula $\ln\underline{p}(\mathbf{y};q)$
\end_inset

 is given in Ormerod and Wand (2010) where also the complete algorithm in
 given as Algorithm 4.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Poission regression 
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
Model
\series default
:
\begin_inset Formula 
\[
Y_{i}|x_{i},\beta\overset{ind.}{\sim}Poisson\left(x_{i}'\beta\right)
\]

\end_inset


\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Prior
\series default
: 
\begin_inset Formula $\beta\sim N(\mu_{\beta},\Sigma_{\beta})$
\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Posterior is intractable.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Assume variational approximation to be Gaussian
\begin_inset Formula 
\[
q_{\beta}(\beta)=N(\mu_{q},\Sigma_{q})
\]

\end_inset


\begin_inset VSpace smallskip
\end_inset


\end_layout

\begin_layout Itemize
Use Newton-Raphson to find 
\begin_inset Formula $\mu_{q},\Sigma_{q}$
\end_inset

 that maximizes the (tractable) lower bound 
\begin_inset Formula $\ln\underline{p}(\mathbf{y};\mu_{q},\Sigma_{q})$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Variational Bayes EM (VBEM)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Latent variable models:
\end_layout

\begin_deeper
\begin_layout Itemize
Model parameters, 
\begin_inset Formula $\theta=(\theta_{1},...,\theta_{p})$
\end_inset


\end_layout

\begin_layout Itemize
Latent variables, 
\begin_inset Formula $\mathbf{z}=(z_{1},...,z_{n})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
Examples:
\end_layout

\begin_deeper
\begin_layout Itemize
Mixture models: 
\begin_inset Formula $z_{i}\in\{1,...,K\}$
\end_inset

 is the mixture allocation for the 
\begin_inset Formula $i$
\end_inset

th observation.
 
\end_layout

\begin_layout Itemize
Missing data: 
\begin_inset Formula $\mathbf{z}$
\end_inset

 contains the missing values.
\end_layout

\end_deeper
\begin_layout Itemize
VBEM approximation of posterior
\begin_inset Formula 
\[
p(\theta,\mathbf{z}|\mathbf{y})\approx q(\theta)q(\mathbf{z})=q(\theta)\prod_{i=1}^{n}q(z_{i})
\]

\end_inset


\end_layout

\begin_layout Itemize
Improves on EM by modelling the uncertainty in 
\begin_inset Formula $\theta$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Indirect use of VB
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
VB can play a role as 
\series bold
initial values for MCMC
\series default
.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Variational MCMC
\series default
.
 Use VB to construct Metropolis-Hastings proposal.
 Need to combine it with Metropolis random walk moves since VB typically
 underestimates the posterior variance.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
VB is useful and accurate for computing 
\series bold
log predictive scores
\series default
 (
\series bold
LPS
\series default
)
\begin_inset Formula 
\[
\sum_{t=T+1}^{T^{*}}\ln p(y_{t}|y_{t-1}^{H})=\sum_{t=T+1}^{T^{*}}\int\ln p(y_{t}|y_{t-1}^{H},\theta)p(\theta|y_{t-1}^{H})d\theta
\]

\end_inset


\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
VB is fast in this setting since it can approximate each sequential posterior
 
\begin_inset Formula $p(\theta|y_{t-1}^{H})$
\end_inset

 using the mode of 
\begin_inset Formula $\hat{p}(\theta|y_{t-2}^{H})$
\end_inset

 as excellent initial values.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
VB seems accurate for approximating LPS, at least when the prediction uncertaint
y is mainly dominated by the future error uncertainty and not by parameter
 uncertainty.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Approximate Bayesian Computations (ABC)
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Suitable when the likelihood is very costly or even infeasible to compute,
 but simulation from the model is cheap.
\end_layout

\begin_layout Itemize
Examples:
\end_layout

\begin_deeper
\begin_layout Itemize
Likelihood is given by an intractable high-dimensional integral
\begin_inset Formula 
\[
\ell(\theta|\mathbf{y})=\int\ell^{*}(\theta|\mathbf{y},\mathbf{u})d\mathbf{u}.
\]

\end_inset


\end_layout

\begin_layout Itemize
Normalizing constant 
\begin_inset Formula $Z_{\theta}$
\end_inset

 is costly or intractable
\begin_inset Formula 
\[
\ell(\theta|\mathbf{y})=\ell_{1}(\theta|\mathbf{y})/Z_{\theta}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Itemize
ABC is often very crude.
\end_layout

\begin_layout Itemize
Arbitrary (creative) choices needed when implementing it.
\end_layout

\begin_layout Itemize
Early days, likely to improve.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood-free rejection sampler 1
\end_layout

\end_inset


\end_layout

\begin_layout Frame

\end_layout

\begin_deeper
\begin_layout Itemize
Idea: 
\begin_inset Formula $\theta$
\end_inset

's with large posterior should generate data 
\begin_inset Formula $\mathbf{z}$
\end_inset

 that look like the actual data 
\begin_inset Formula $\mathbf{y}$
\end_inset

.
\end_layout

\begin_layout Itemize
Assume the data 
\begin_inset Formula $\mathbf{y}$
\end_inset

 takes values in a finite or countable set 
\begin_inset Formula $\mathcal{D}$
\end_inset

.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
for
\series default
 
\begin_inset Formula $i=1$
\end_inset

 to 
\begin_inset Formula $N$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
Generate 
\begin_inset Formula $\theta'$
\end_inset

 from the prior distribution 
\begin_inset Formula $\pi(\cdot)$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $\mathbf{z}$
\end_inset

 from the data distribution 
\begin_inset Formula $f(\cdot|\theta')$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
until
\series default
 
\begin_inset Formula $\mathbf{z}=\mathbf{y}$
\end_inset


\end_layout

\begin_layout Itemize
set 
\begin_inset Formula $\theta_{i}=\theta'$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end for
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Algorithm 1 produces a sample 
\begin_inset Formula $\theta_{1},...,\theta_{N}$
\end_inset

 from the posterior 
\begin_inset Formula $\pi(\theta|\mathbf{y})$
\end_inset

:
\begin_inset Formula 
\[
f(\theta_{i})\propto\sum_{\mathbf{z}\in\mathcal{D}}\pi(\theta_{i})f(\mathbf{z}|\theta_{i})\mathbb{I}_{\mathbf{y}}(\mathbf{z})=\pi(\theta_{i})f(\mathbf{y}|\theta_{i})\propto\pi(\theta_{i}|\mathbf{y}).
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood-free rejection sampler 2
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Extension to continuous sample spaces where 
\begin_inset Formula $Pr(\mathbf{z}=\mathbf{y}|\theta)=0$
\end_inset

.
\end_layout

\begin_layout Itemize
Define summary statistics 
\begin_inset Formula $\eta(\mathbf{z})$
\end_inset

 and a distance function 
\begin_inset Formula $\rho\left[\eta(\mathbf{z}),\eta(\mathbf{y})\right]$
\end_inset

.
 
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
for
\series default
 
\begin_inset Formula $i=1$
\end_inset

 to 
\begin_inset Formula $N$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
repeat
\end_layout

\begin_deeper
\begin_layout Itemize
Generate 
\begin_inset Formula $\theta'$
\end_inset

 from the prior distribution 
\begin_inset Formula $\pi(\cdot)$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $\mathbf{z}$
\end_inset

 from the data distribution 
\begin_inset Formula $f(\cdot|\theta')$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
until
\series default
 
\begin_inset Formula $\rho\left[\eta(\mathbf{z}),\mathbf{\eta}(\mathbf{y})\right]\leq\varepsilon$
\end_inset


\end_layout

\begin_layout Itemize
set 
\begin_inset Formula $\theta_{i}=\theta'$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end for
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
Algorithmic choices
\series default
: 
\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $\eta$
\end_inset

 - a function on 
\begin_inset Formula $\mathcal{D}$
\end_inset

 defining a summary statistic (close to sufficient)
\end_layout

\begin_layout Itemize
\begin_inset Formula $\rho>0$
\end_inset

, a distance on 
\begin_inset Formula $\eta(\mathcal{D})$
\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\varepsilon>0$
\end_inset

, a tolerance level.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Likelihood-free rejection sampler 2, cont.
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
The algorithm samples from the joint distribution
\begin_inset Formula 
\[
\pi_{\varepsilon}(\theta,\mathbf{z}|\mathbf{y})=\frac{\pi(\theta)f(\mathbf{z}|\theta)\mathbb{I}_{A_{\varepsilon,\mathbf{y}}}(\mathbf{z})}{\int_{A_{\varepsilon,\mathbf{y}}\times\theta}\pi(\theta)f(\mathbf{z}|\theta)\mathbb{I}_{A_{\varepsilon,\mathbf{y}}}(\mathbf{z})d\mathbf{z}d\theta}
\]

\end_inset

where 
\begin_inset Formula 
\[
A_{\varepsilon,\mathbf{y}}=\left\{ \mathbf{z}\in\mathcal{D}|\rho\left[\eta(\mathbf{z}),\mathbf{\eta}(\mathbf{y})\right]\leq\varepsilon\right\} 
\]

\end_inset


\end_layout

\begin_layout Itemize
The hope is that 
\begin_inset Formula 
\[
\pi(\theta|\mathbf{y})\approx\pi_{\varepsilon}(\theta|\mathbf{y})=\int\pi_{\varepsilon}(\theta,\mathbf{z}|\mathbf{y})d\mathbf{z}
\]

\end_inset

is a good approximation.
\end_layout

\begin_layout Itemize
\begin_inset Quotes eld
\end_inset


\emph on
The basic idea behind ABC is that using a representative (enough) summary
 statistic 
\begin_inset Formula $\eta$
\end_inset

 coupled with a small (enough) tolerance 
\begin_inset Formula $\varepsilon$
\end_inset

 should produce a good (enough) approximation to the posterior distribution
\emph default

\begin_inset Quotes erd
\end_inset

 (Marin et al, 2012).
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
ABC - an example
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
\begin_inset Formula $MA(q)$
\end_inset

 model.
 Fairly complicated likelihood.
 Easy to simulate time series from 
\begin_inset Formula $MA(q)$
\end_inset

.
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize
Summary statistics:
\end_layout

\begin_deeper
\begin_layout Itemize
Raw distance between time series: 
\begin_inset Formula $\rho\left[(z_{1},...,z_{n}),(y_{1},...,y_{n})\right]=\sqrt{\sum_{i=1}^{n}(y_{i}-z_{i})^{2}}$
\end_inset


\end_layout

\begin_layout Itemize
Distance between estimated autocorrelation functions: 
\begin_inset Formula $\sum_{j=1}^{K}(\tau_{y,j}-\tau_{z,j})^{2}$
\end_inset

.
\end_layout

\end_deeper
\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
MCMC - ABC
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Likelihood-free rejection sampler 2 is inefficient since it proposes 
\begin_inset Formula $\theta$
\end_inset

's from the prior 
\begin_inset Formula $\pi(\theta)$
\end_inset

, which is often far from the posterior (with informative data).
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Initialize 
\begin_inset Formula $(\mbox{\theta}^{(0)},\mathbf{z}^{(0)})$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
for
\series default
 
\begin_inset Formula $i=1$
\end_inset

 to 
\begin_inset Formula $N$
\end_inset

 do
\end_layout

\begin_deeper
\begin_layout Itemize
Propose 
\begin_inset Formula $\theta'$
\end_inset

 from the Markov kernel 
\begin_inset Formula $q\left(\cdot|\theta^{(t-1)}\right)$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $\mathbf{z}'$
\end_inset

 from the data distribution 
\begin_inset Formula $f(\cdot|\theta')$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $u$
\end_inset

 from 
\begin_inset Formula $\mathcal{U}_{[0,1]}$
\end_inset


\end_layout

\begin_layout Itemize

\series bold
if
\series default
 
\begin_inset Formula $u\leq\frac{\pi\left(\theta'\right)q\left(\theta^{(t-1)}|\theta'\right)}{\pi\left(\theta^{(t-1)}\right)q\left(\theta'|\theta^{(t-1)}\right)}$
\end_inset

 and 
\begin_inset Formula $\rho\left[\eta(\mathbf{z}'),\eta(\mathbf{y})\right]\leq\varepsilon$
\end_inset

 
\series bold
then
\end_layout

\begin_deeper
\begin_layout Itemize
set 
\begin_inset Formula $(\theta^{(t)},\mathbf{z}^{(t)})=(\theta',\mathbf{z}')$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
else
\end_layout

\begin_deeper
\begin_layout Itemize
set 
\begin_inset Formula $(\theta^{(t)},\mathbf{z}^{(t)})=(\theta^{(t-1)},\mathbf{z}^{(t-1)})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end if
\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end for
\begin_inset VSpace medskip
\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Noisy ABC
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Replacing the crude 
\begin_inset Formula $\rho\left[\eta(\mathbf{z}),\eta(\mathbf{y})\right]\leq\varepsilon$
\end_inset

 rejection rule with a smoother version
\begin_inset Formula 
\[
\pi_{\varepsilon}(\theta,\mathbf{z}|\mathbf{y})=\frac{\pi(\theta)f(\mathbf{z}|\theta)K_{\varepsilon}(\mathbf{y}-\mathbf{z})}{\int\pi(\theta)f(\mathbf{z}|\theta)K_{\varepsilon}(\mathbf{y}-\mathbf{z})d\mathbf{z}d\theta}
\]

\end_inset

where 
\begin_inset Formula $K_{\varepsilon}(\cdot)$
\end_inset

 is a kernel (think normal density) parametrized by the bandwidth 
\begin_inset Formula $\varepsilon>0$
\end_inset

.
\end_layout

\begin_layout Itemize
The Bayes estimator from 
\begin_inset Formula $\pi_{\varepsilon}(\theta|\mathbf{y})$
\end_inset

 is converging to the true value when 
\begin_inset Formula $n\rightarrow\infty$
\end_inset

 and 
\begin_inset Formula $\varepsilon\rightarrow0$
\end_inset

.
\end_layout

\begin_layout Itemize
See Wilkinson (2008) for details about the algorithm.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Some common kernels
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Standard
\align center
\begin_inset Graphics
	filename Figures/Kernels.svg
	scale 50

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Choosing the algoritmic settings in ABC
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Summary statistics 
\begin_inset Formula $\eta(\cdot)$
\end_inset

 should be nearly sufficient.
 Which ones? Creativity ...
\end_layout

\begin_layout Itemize
Choice of 
\begin_inset Formula $\eta(\cdot)$
\end_inset

 is crucial.
\end_layout

\begin_layout Itemize
Choice of 
\begin_inset Formula $\varepsilon$
\end_inset

 is less important.
 Smaller 
\begin_inset Formula $\varepsilon$
\end_inset

 gives better approximation at higher computational cost.
\end_layout

\begin_layout Itemize
Common choice of 
\begin_inset Formula $\varepsilon$
\end_inset

: small (
\begin_inset Formula $0.1$
\end_inset

% or so) percentile of simulated distances
\begin_inset Formula $\rho\left[\eta(\mathbf{z}'),\eta(\mathbf{y})\right]$
\end_inset

.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
Post-processing of ABC output
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
Sample algorithms, but allowing for larger 
\begin_inset Formula $\varepsilon$
\end_inset

 by post-processing the ABC output.
\end_layout

\begin_layout Itemize
Do not reject 
\begin_inset Formula $\theta$
\end_inset

 draws that generate 
\begin_inset Formula $\mathbf{z}$
\end_inset

 far from the actual data, but shrink the 
\begin_inset Formula $\theta$
\end_inset

 draws using the (multivariate?) regression
\begin_inset Formula 
\[
\theta*=\theta-\left(\eta(\mathbf{z})-\eta(\mathbf{y})\right)'\hat{\beta}
\]

\end_inset


\end_layout

\begin_layout Itemize
\begin_inset Formula $\hat{\beta}$
\end_inset

 is obtained from a local kernel regression of 
\begin_inset Formula $\theta$
\end_inset

 on 
\begin_inset Formula $\rho\left[\eta(\mathbf{z}),\eta(\mathbf{y})\right]$
\end_inset

 with weights given by the kernel
\begin_inset Formula 
\[
K_{\delta}\left\{ \rho\left[\eta(\mathbf{z}),\eta(\mathbf{y})\right]\right\} .
\]

\end_inset


\end_layout

\begin_layout Itemize
Kernel bandwidth 
\begin_inset Formula $\delta$
\end_inset

 can for example be set equal to ABC tolerance 
\begin_inset Formula $\varepsilon$
\end_inset

.
\end_layout

\begin_layout Itemize
Alternative: heteroscedastic nonlinear regression.
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
ABC for model choice
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize
You are entertaining a 
\series bold
set of 
\begin_inset Formula $M$
\end_inset

 different
\series default
 (competing, possibly non-nested) 
\series bold
models
\series default
.
 
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\text{\mathcal{M}}$
\end_inset

 denote the unknown true model, and let 
\begin_inset Formula $\pi(\mathcal{M}=m)$
\end_inset

 denote the 
\series bold
prior distribution
\series default
 over the 
\series bold
model space
\series default
.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
The 
\series bold
Bayesian solution for model inference
\series default
: the posterior distribution: 
\begin_inset Formula $\pi(\mathcal{M}=m|\mathbf{y})$
\end_inset

.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
ABC solution
\series default
: include 
\begin_inset Formula $\mathcal{M}$
\end_inset

 in the set of parameters.
\begin_inset VSpace medskip
\end_inset


\end_layout

\begin_layout Itemize
Let 
\begin_inset Formula $\eta(\mathbf{z})=\left(\eta_{1}(\mathbf{z}),...,\eta_{M}(\mathbf{z})\right)$
\end_inset

 be the concatenation of the 
\series bold
summary statistics
\series default
 used for all models.
 
\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\begin_layout Frame
\begin_inset Argument 4
status open

\begin_layout Plain Layout
ABC algorithm for model choice
\end_layout

\end_inset


\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
for
\series default
 
\begin_inset Formula $i=1$
\end_inset

 to 
\begin_inset Formula $N$
\end_inset

 
\series bold
do
\end_layout

\begin_deeper
\begin_layout Itemize

\series bold
repeat 
\end_layout

\begin_deeper
\begin_layout Itemize
Generate 
\begin_inset Formula $m$
\end_inset

 from the prior 
\begin_inset Formula $\pi(\mathcal{M}=m)$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $\theta_{m}$
\end_inset

 from the prior 
\begin_inset Formula $\pi_{m}(\theta_{m})$
\end_inset


\end_layout

\begin_layout Itemize
Generate 
\begin_inset Formula $\mathbf{z}$
\end_inset

 from the data distribution 
\begin_inset Formula $f_{m}(\cdot|\theta_{m})$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
until 
\series default

\begin_inset Formula $\rho\left[\eta(\mathbf{z}),\eta(\mathbf{y})\right]\leq\varepsilon$
\end_inset

 
\end_layout

\begin_layout Itemize
Set 
\begin_inset Formula $m^{(i)}=m$
\end_inset

 and 
\begin_inset Formula $\theta^{(i)}=\theta_{m}$
\end_inset


\end_layout

\end_deeper
\begin_layout Itemize

\series bold
end for
\end_layout

\begin_layout Standard
\begin_inset VSpace bigskip
\end_inset


\end_layout

\begin_layout Itemize

\series bold
ABC estimate
\series default

\begin_inset Formula 
\[
\pi(\mathcal{M}=m|\mathbf{y})\approx\frac{1}{N}\sum_{i=1}^{N}\mathbb{I}_{m^{(i)}=m}
\]

\end_inset


\end_layout

\end_deeper
\begin_layout Separator

\end_layout

\end_body
\end_document
